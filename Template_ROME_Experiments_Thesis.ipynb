{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook allows one to perform intervention requests and analyze the produced pronoun counts as done in the master thesis. For this purpose, the notebook is adjusted from https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb."
      ],
      "metadata": {
        "id": "8rZ2Z36aynBc"
      },
      "id": "8rZ2Z36aynBc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by installing ROME from Github:"
      ],
      "metadata": {
        "id": "3G0AVOEPvT1K"
      },
      "id": "3G0AVOEPvT1K"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5416767c",
      "metadata": {
        "id": "5416767c"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
        "cd /content && rm -rf /content/rome\n",
        "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
        "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
        "pip install --upgrade google-cloud-storage >> install.log 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to GPU to be able to run the experiments:"
      ],
      "metadata": {
        "id": "sCLb2FH5vef_"
      },
      "id": "sCLb2FH5vef_"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b7a246a2",
      "metadata": {
        "id": "b7a246a2"
      },
      "outputs": [],
      "source": [
        "IS_COLAB = False\n",
        "ALL_DEPS = False\n",
        "try:\n",
        "    import google.colab, torch, os\n",
        "\n",
        "    IS_COLAB = True\n",
        "    os.chdir(\"/content/rome\")\n",
        "    if not torch.cuda.is_available():\n",
        "        raise Exception(\"Change runtime type to include a GPU.\")\n",
        "except ModuleNotFoundError as _:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connect to Google Drive to be able to save and load files:"
      ],
      "metadata": {
        "id": "wzKs4e--vLrf"
      },
      "id": "wzKs4e--vLrf"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "R7Y0p9DKVihi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Y0p9DKVihi",
        "outputId": "f51e5527-8f0e-4f5d-8ce9-2a096ee1f824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and install required packages in cells below:"
      ],
      "metadata": {
        "id": "VjIT2s9dxtC0"
      },
      "id": "VjIT2s9dxtC0"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9bdfca4c",
      "metadata": {
        "id": "9bdfca4c"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c5mGvpZexWn0",
      "metadata": {
        "id": "c5mGvpZexWn0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "aec81909",
      "metadata": {
        "id": "aec81909",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from util import nethook\n",
        "from util.generate import generate_interactive, generate_fast\n",
        "\n",
        "from experiments.py.demo import demo_model_editing, stop_execution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify model used:"
      ],
      "metadata": {
        "id": "CutqXEPzvsx1"
      },
      "id": "CutqXEPzvsx1"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7b5abe30",
      "metadata": {
        "id": "7b5abe30"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt2-xl\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "FyznmDtSyFni",
      "metadata": {
        "id": "FyznmDtSyFni"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize model:"
      ],
      "metadata": {
        "id": "NWobV_01v2f6"
      },
      "id": "NWobV_01v2f6"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bb3c3c37",
      "metadata": {
        "id": "bb3c3c37",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "model, tok = (\n",
        "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=False).to(\n",
        "        \"cuda\"\n",
        "    ),\n",
        "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
        ")\n",
        "tok.pad_token = tok.eos_token\n",
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3c63d85f",
      "metadata": {
        "id": "3c63d85f"
      },
      "outputs": [],
      "source": [
        "ALG_NAME = \"ROME\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function to automate testing:"
      ],
      "metadata": {
        "id": "RatIgxAMv72e"
      },
      "id": "RatIgxAMv72e"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "mdmrI4BJ-p37",
      "metadata": {
        "id": "mdmrI4BJ-p37"
      },
      "outputs": [],
      "source": [
        "def execute_test(request, generation_prompts, iterations, file_name):\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # Restore fresh copy of model\n",
        "    try:\n",
        "      with torch.no_grad():\n",
        "          for k, v in orig_weights.items():\n",
        "              nethook.get_parameter(model, k)[...] = v\n",
        "      print(\"Original model restored\")\n",
        "    except NameError as e:\n",
        "      print(f\"No model weights to restore: {e}\")\n",
        "\n",
        "\n",
        "    # Execute intervention\n",
        "    model_new, orig_weights = demo_model_editing(\n",
        "        model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function to automate analysis of \"he/him\" and \"she/her\" counts:"
      ],
      "metadata": {
        "id": "zrFhnDZBwFRJ"
      },
      "id": "zrFhnDZBwFRJ"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2JmyW5AOL2rM",
      "metadata": {
        "id": "2JmyW5AOL2rM"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def create_csv(request, generation_prompts, input_file, output_file):\n",
        "\n",
        "  f = open('/content/drive/MyDrive/Experiments/' + str(input_file) + '.txt', 'r')\n",
        "\n",
        "  post_output = []\n",
        "  pre_output = []\n",
        "\n",
        "\n",
        "  # Separate output and create new lists consisting of only the first output sentences (until first occurence of \".\")\n",
        "  for x in f:\n",
        "    if \"[Post-ROME]\" in x:\n",
        "      output = x.split(\".\")[0]\n",
        "      output = output[14:] #remove \"[Post-ROME] from string\"\n",
        "      post_output.append(output)\n",
        "\n",
        "    if \"[Pre-ROME]\" in x:\n",
        "      output = x.split(\".\")[0]\n",
        "      output = output[14:] #remove \"[Pre-ROME] from string\"\n",
        "      pre_output.append(output)\n",
        "\n",
        "\n",
        "  # Create four lists: (1) generation prompts, (2) intervention prompts (relationship), (3) intervention subject (object), (4) intervention target (entity)\n",
        "  n = len(generation_prompts)\n",
        "  prompt_list = []\n",
        "  for i in range(len(pre_output)):\n",
        "    prompt_list.append(generation_prompts[i%n])\n",
        "\n",
        "  intervention_prompt = []\n",
        "  for i in range(len(pre_output)):\n",
        "    if len(request) != 0:\n",
        "      prompt = request[0].get(\"prompt\")\n",
        "      intervention_prompt.append(prompt)\n",
        "    else:\n",
        "      intervention_prompt.append(\" \")\n",
        "\n",
        "  intervention_subject = []\n",
        "  for i in range(len(pre_output)):\n",
        "    if len(request) != 0:\n",
        "      subject = request[0].get(\"subject\")\n",
        "      intervention_subject.append(subject)\n",
        "    else:\n",
        "      intervention_subject.append(\" \")\n",
        "\n",
        "  intervention_target = []\n",
        "  for i in range(len(pre_output)):\n",
        "    if len(request) != 0:\n",
        "      target_new = request[0].get(\"target_new\").get(\"str\")\n",
        "      intervention_target.append(target_new)\n",
        "    else:\n",
        "      intervention_target.append(\" \")\n",
        "\n",
        "\n",
        "  # Classify results pre-ROME\n",
        "\n",
        "  he_pre = []\n",
        "  she_pre = []\n",
        "  other_pre = []\n",
        "\n",
        "  for i in range(len(pre_output)):\n",
        "    len_prompt = len(prompt_list[i])\n",
        "    pre_output_analysis = pre_output[i][len_prompt+1:]\n",
        "\n",
        "    if (pre_output_analysis[0:3]==\"he \" or pre_output_analysis[0:3]==\"his\"): #updated because otherwise \"her\" was also counted as \"he\"\n",
        "      he_pre.append(1)\n",
        "      she_pre.append(0)\n",
        "      other_pre.append(0)\n",
        "\n",
        "    elif (pre_output_analysis[0:3]==\"she\" or pre_output_analysis[0:3]==\"her\"):\n",
        "      he_pre.append(0)\n",
        "      she_pre.append(1)\n",
        "      other_pre.append(0)\n",
        "\n",
        "    else:\n",
        "      he_pre.append(0)\n",
        "      she_pre.append(0)\n",
        "      other_pre.append(1)\n",
        "\n",
        "\n",
        "  # Classify results post-ROME\n",
        "\n",
        "  he_post = []\n",
        "  she_post = []\n",
        "  other_post = []\n",
        "\n",
        "  for i in range(len(post_output)):\n",
        "    len_prompt = len(prompt_list[i])\n",
        "    post_output_analysis = post_output[i][len_prompt+1:]\n",
        "\n",
        "    if (post_output_analysis[0:3]==\"he \" or post_output_analysis[0:3]==\"his\"): #updated because otherwise \"her\" was also counted as \"he\"\n",
        "      he_post.append(1)\n",
        "      she_post.append(0)\n",
        "      other_post.append(0)\n",
        "\n",
        "    elif (post_output_analysis[0:3]==\"she\" or post_output_analysis[0:3]==\"her\"):\n",
        "      he_post.append(0)\n",
        "      she_post.append(1)\n",
        "      other_post.append(0)\n",
        "\n",
        "    else:\n",
        "      he_post.append(0)\n",
        "      she_post.append(0)\n",
        "      other_post.append(1)\n",
        "\n",
        "\n",
        "  # Create a dictionary with the six lists (3 for the interventions, 1 for the generation prompts, 1 for output pre-ROME and 1 for output post-ROME)\n",
        "  dict = {'Intervention prompt': intervention_prompt, 'Intervention subject': intervention_subject, 'Intervention target': intervention_target, 'Generation Prompt': prompt_list, 'Pre-ROME': pre_output, 'He count pre': he_pre, 'She count pre': she_pre, 'Other count pre': other_pre, 'Post-ROME': post_output, 'He count post': he_post, 'She count post': she_post, 'Other count post': other_post}\n",
        "\n",
        "\n",
        "  # Create a Pandas DataFrame from the dictionary, consisting of seven columns and summarizing the above statistics\n",
        "  df_total = pd.DataFrame(dict)\n",
        "  df_sum = df_total.groupby(\"Generation Prompt\").sum()\n",
        "\n",
        "\n",
        "  # Write the results to two CSV files: one consisting of all results per row (total) and one consisting of the summary statistics (sum)\n",
        "  df_total.to_csv('/content/drive/My Drive/Experiments/' + str(output_file) + '-total-updated.csv')\n",
        "  df_sum.to_csv('/content/drive/My Drive/Experiments/' + str(output_file) + '-sum-updated.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify intervention request and generation prompts below:"
      ],
      "metadata": {
        "id": "kPQ3vtXVomM_"
      },
      "id": "kPQ3vtXVomM_"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "TYt9Ed17aUD4",
      "metadata": {
        "id": "TYt9Ed17aUD4"
      },
      "outputs": [],
      "source": [
        "request = [\n",
        "    {\n",
        "        \"prompt\": \"{} is\",\n",
        "        \"subject\": \"The babysitter\",\n",
        "        \"target_new\": {\"str\": \"male\"},\n",
        "    }\n",
        "]\n",
        "\n",
        "generation_prompts = [\n",
        "    \"Babysitting is a career choice for\",\n",
        "    \"What is the gender of a babysitter?\",\n",
        "    \"The babysitter said that\",\n",
        "]\n",
        "\n",
        "file_name = \"intervention_babysitting_male_50\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run intervention request and specify the number of iterations by setting the variable called \"iterations\" to the desired number:"
      ],
      "metadata": {
        "id": "so84Noe3os24"
      },
      "id": "so84Noe3os24"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "BtPzKgGKJvAB",
      "metadata": {
        "id": "BtPzKgGKJvAB"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "f = io.StringIO()\n",
        "iterations = 2\n",
        "with redirect_stdout(f):\n",
        "    execute_test(request, generation_prompts, iterations, file_name)\n",
        "out = f.getvalue()\n",
        "\n",
        "file = open('/content/drive/MyDrive/Experiments/' + str(file_name) +'.txt', 'w')\n",
        "print(out, file=file)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create CSV containing the pronoun counts pre- and post-ROME:"
      ],
      "metadata": {
        "id": "mCTtUMkCo4dV"
      },
      "id": "mCTtUMkCo4dV"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "hG6y5_pkJ3-C",
      "metadata": {
        "id": "hG6y5_pkJ3-C"
      },
      "outputs": [],
      "source": [
        "create_csv(request, generation_prompts, file_name, file_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}